from env import graph_env
import QAgent

# Numpy
import numpy as np


def distance(agent1, agent2, observations):
    """
    Returns the distance between two agents in a given observation
    :param agent1: String representation of agent1
    :param agent2: String representation of agent2
    :param observations: Dictionary of observations
    :return: distance
    """
    return np.sqrt((observations[agent1]['x'] - observations[agent2]['x']) ** 2 + (
            observations[agent1]['y'] - observations[agent2]['y']) ** 2)


def partition_circle_act(target_agent, action, observations, env, r, partitions):
    """
    This returns a vector that represents whether an agent is within radius r of the target agent.
    If it is within the radius, then the vector represents the count that corresponds to each partition.
    Ex: (1, 0, 2, 1) means 1 object is within radius r and in first quadrant, 0 objects in second quadrant...
    :param target_agent: target agent centered around
    :param action: the action that the target agent will take
    :param observations: observations for a given step
    :param env: current environment
    :param r: radius of the circle
    :param partitions: number of partitions
    :return: vector of counts
    """
    # Do the action step!
    # Process each agent's action
    displacement = [[-1, -1], [0, -1], [1, -1], [-1, 0], [0, 0], [1, 0], [-1, 1], [0, 1], [1, 1]]
    delta = 0.1
    next_x = observations[target_agent]["x"] + displacement[action][0] * delta
    next_y = observations[target_agent]["y"] + displacement[action][1] * delta

    vec_counts = np.zeros(partitions)

    for agent in env.agents:
        if agent != target_agent:
            agent_x = observations[agent]['x']
            agent_y = observations[agent]['y']
            # Note: Code below generated by chatgpt then checked and modified by me
            # Convert Cartesian coordinates to polar coordinates
            radius, theta = distance(target_agent, agent, observations), np.arctan2((agent_y - next_y),
                                                                                    (agent_x - next_x))

            # Normalize theta to be in the range [0, 2*pi)
            theta = (theta + 2 * np.pi) % (2 * np.pi)
            # Calculate the sector size for six partitions
            sector_size = 2 * np.pi / partitions

            # Determine the partition index
            partition_index = np.floor(theta / sector_size).astype(int)

            # Only add if within distance
            if radius <= r:
                vec_counts[partition_index] += 1
    return vec_counts


def distToKNearestNeighborsWithEdgeLength(target_agent, action, observations, env, k, idealEdgeLength):
    """
    Returns the distance between two agents in a given observation
    :param target_agent: String representation of target_agent
    :param action: action taken by the target agent
    :param observations: observations of the state
    :param env: environment of graph
    :param k: number of neighbors
    :return: distance as a vector, with the first being the closest
    """

    displacement = [[-1, -1], [0, -1], [1, -1], [-1, 0], [0, 0], [1, 0], [-1, 1], [0, 1], [1, 1]]
    delta = 0.5
    next_x = observations[target_agent]["x"] + displacement[action][0] * delta
    next_y = observations[target_agent]["y"] + displacement[action][1] * delta

    dist_dict = {}
    for agent in env.agents:
        if agent != target_agent:
            dist_dict[agent] = ((np.sqrt((next_x - observations[agent]['x']) ** 2 +
                                         (next_y - observations[agent]['y']) ** 2)) - idealEdgeLength) ** 2

    if k > 1:
        dists = np.sort(np.fromiter(dist_dict.values(), dtype=float))
        return dists[:k]
    else:
        dists = np.fromiter(dist_dict.values(), dtype=float)
        return dists[0]


env = graph_env.parallel_env(render_mode="human")
# observations, infos = env.reset(options={"elements": {"nodes": [
#     {"data": {"id": '1'}},
#     {"data": {"id": '0'}},
#     {"data": {"id": '2'}}
# ], "edges": [
#     {"data": {"source": '0', "target": '1', "directed": 'false'}},
#     {"data": {"source": '1', "target": '2', "directed": 'false'}}
# ]},
#     "randomInit": True,
# })
observations, infos = env.reset(options={"elements": {
    "nodes": [
        {"data": {"id": 'v1'}},
        {"data": {"id": 'v2'}},
        {"data": {"id": 'v3'}},
        {"data": {"id": 'v4'}},
        {"data": {"id": 'v5'}},
        {"data": {"id": 'v6'}},
        {"data": {"id": 'v0'}}
    ],
    "edges": [
        {"data": {"source": 'v0', "target": 'v1'}},
        {"data": {"source": 'v0', "target": 'v2'}},
        {"data": {"source": 'v0', "target": 'v3'}},
        {"data": {"source": 'v1', "target": 'v4'}},
        {"data": {"source": 'v2', "target": 'v5'}},
        {"data": {"source": 'v3', "target": 'v6'}},
    ]
}, "randomInit": True})

# Create an agent
learner = QAgent.QLearningAgent(env=env, epsilon=0.05, gamma=0.8, alpha=0.2,
                                features=[partition_circle_act, distToKNearestNeighborsWithEdgeLength,
                                          8])  # Last number is total number of features

while env.agents:
    states = {agent: {'agent': agent, 'observations': observations, 'env': env} for agent in env.agents}
    # actions = {agent: env.action_space(agent).sample() for agent in env.agents}
    actions = {agent: learner.getAction(states[agent]) for agent in env.agents}
    observations, rewards, terminations, truncations, infos = env.step(actions)
    # print(rewards)
    # print("Action:", actions)
    # print(distance("agent_0", "agent_1", observations))
    # (distance("agent_1", "agent_2", observations))
    print(observations)
    nextStates = {agent: {'agent': agent, 'observations': observations, 'env': env} for agent in env.agents}
    for agent in env.agents:
        learner.update(states[agent], actions[agent], nextStates[agent], rewards[agent])
env.close()
